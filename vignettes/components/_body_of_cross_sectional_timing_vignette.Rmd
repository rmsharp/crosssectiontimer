
# Introduction

```{r child = here::here("vignettes/components/_contents.Rmd"), ref = "contents"}

```

```{r child = here::here("vignettes/components/_theoretical_background.Rmd"), ref = "theory"}

```


## Purpose

Management of a value stream requires adequate estimates of the resources used
to create value, with time on task being a direct measure of the primary
resource^[Employee effort as measured by time is the most expensive resource.
In addition, creating a work environment the encourages retention of employees
requires time management through appropriate prioritization so that employees
are maximally productive and fulfilled.] creating the value.  The need to
obtain quality estimates of these times on the various tasks in a value stream
is the motivation to provide these tools and to advocate for the use of this
cross-sectional time on task estimation method.

In the following example, estimates are obtained of how much time is spent on
specific activities by various actors^[
`r get_and_or_list(unique(activities$actor))`] responsible for the success of 
the `r value_stream_descriptor`. 
Time-on-task data provide a fundamental metric used to identify those areas
where improvement opportunities exist.

Detailed planning regarding questions to be addressed is required prior to 
creating the list of activities to be time. 
(See [Selection of Activities to be Timed].)

A benefit of time on task studies is that they provide hard data regarding how
time is spent and how much of it directly contributes to our primary goals. A
second benefit is that these data allow each class of actor to advocate for
modifying the way their time is being used to improve their work environment
and productivity.

## Comparison of Some Time Analysis Approaches

Studies that measure duration between value stream milestones have the following
weaknesses.

-   Good estimates of duration between value stream milestones require multiple
    observations per milestone interval and observing the entire value stream
    (entire duration) multipole times.
    -  This means little information is obtained until multiple full passes 
       through the value stream have occurred. Thus, no data driven improvements
       can be implemented while waiting for p=completion of those complete 
       passers to occur. This represent an unacceptable lost opportunity cost 
       for those value streams taking any approciable amount of time.
-   Measures of duration between milestones provide no insight into
    percent effort expended to create value within the duration.
    -  Thus, once you have estimates of duration between milestones, you 
       remain blind to how the resources were being used within that duration.
-   Milestones do not provide detail available through use of specific tasks.
    -  Milestones, by there nature, are reached through multiple actions of 
       a few to many different types. The percent of effort and the pattern
       of effort in those actions remains unknown.
-   Milestones impose a linear view of work while the value stream may contain 
    several important feedback loops.

Longitudinal time-on-task estimation methods that call for representative 
actors within each group to log time on tasks during their work day have the
weaknesses listed below.

-   Failure rate in logging activities can be high.
-   Recall of the amount of time spent is often wrong, resulting in an
    artificially inflated variance.
-   Preconceptions of how time should be spent biases recalled durations.
-   Keeping, collecting, and analyzing logs requires significant administrative
    overhead.
-   The biases described in earlier points makes interpretation of results
    difficult.
-   The longitudinal collection of data requires a long data collection
    phase before any analyses can begin.
-   Variance among different projects is anticipated to be high, which
    further complicates analysis.


The MTS design presented^[The MTS design assumes that we have the ability to 
send a question to selected individuals at selected times and collect that 
response.] and tested below has the following advantages.

-   The _failure to respond rate_ has far less impact since failure to respond
    simply initiates another response request.
-   Participants do not need to recall time spent.
-   Administrative overhead is greatly reduced.
-   Bias is minimized.
-   Data can be analyzed at any time.
-   Variance among projects could be assessed and more accurately measured if
    desired. However, I recommend that project differences be ignored during
    initial timing studies.

# Example Measurement Protocol

Detailed planning regarding questions to be addressed and how and when those
questions are presented to participants is required for the successful use of
this time on task measurement protocol.

## Selection of Activities to be Timed

Creation of the activity list is a critical step. The process should engage all
participants to included everything that should be measured is and leave out
those that should not be measured. The goal is to discover how the actors' work
environment can be improved. Thus, measuring time spent on activities not
directly contributing to the value stream(s) being studied are often critical.
Examples, include administrative activities, non-mission critical meetings, and
dealing with work blocking technical issues.

One or more individuals familiar with all potential workday activities should
make the initial trial list. This list should then be submitted to participants
for further refinement. Having the participants help create and refine the list
is critical to assuring good coverage of possible activities and to reinforce
the nature of this as a team effort to gain insight for workplace enhancement
and not as an imposed activity designed to identify poor work habits. Further,
it is preferable that the management groups of the primary actors take an
analogous time on task measurement sample during the same time period and
publish their findings at the same time.

Some actors will work on more than one value stream and all will have
additional responsibilities specific to their work environment. Thus, some
activities recorded are intentionally defined as various unrelated activities.
The activity list could include __Personal__ to assure participants that
management supports a healthy work-life balance.  This is an example where the
inclusion of activities not directly related to the workstream under study may
be appropriate. These activities could include references to practices
encouraged by management such as ongoing education, exercise breaks, and
helping co-workers. It is critical that these aspects of the work landscape are
clearly described so that any negative stigma associated with non-workstream
related activities are accurately captured.

Finding that the activity equivalent to __Other__ in the simulation below
represents more time than anticipated may be a reason to collect additional
information to identify sub components.

## Collecting the Timing Data

The method used to collect the timing data should have minimal impact on the
actor's day.  An ideal would be to have a modal box appear on an actor's
computer monitor and request a minimal amount of input so that the modal box
can be dismissed in less than 10 seconds where fewer seconds are desirable.

A webpage could be used by all participants to indicate a willingness to
participate, their current availability, and provide any basic metadata needed
to ensure the correct activities list is provided and the data collected is
appropriately aggregated for analysis.

This webpage could interact via a web API to a background application that
contained the various activity lists being used, an algorithm for selecting
when actors are prompted for a current activity response, and a method to
accept the response data and store it in a database that can later be accessed
for data analysis.

Group chat tools, such as __Slack__^[Slack is an online communication platform
for teams] may have APIs to provide similar services.^[See,
https://api.slack.com/block-kit/building for a possible Slack approach.]

It is recommended that uniformly pseudo-random times throughout the work period
(typically day, week, and month) would be used so that systematic work
schedules will not bias the results.

## Data Analysis and Followup

Planning how much detail to collect and how the data will be analyzed should
occur prior to any data being collected.  This essential to preventing bias
introduced by observing the results prior to designing the analysis.

The primary analysis should include calculating the time estimates for each
activity.  $$time_{activity_i} = m_i * time\_units / \sum_{i=1}^{i=n} m_i$$
where $i$ is the index of the activity of interest, $time\_units$ is the total
number of units of the desired unit of measure within the time period of
interest, such as 560 minutes in a typical 8 hour work day, $m_i$ is the number
of times activity $i$ was observed, $n$ is the number of distinct activities.

See the [description of Table \@ref(tab:small-sample-in-minutes)](
#calculation-of-minutes-per-day) for an example where the calculation of
minutes per activity in one 8 hour day is illustrated.

Prior to the collection of data the various questions to be addressed should be
articulated along with a description of how the provided activity durations
will address those questions. In addition to questions to be addressed,
expected relationships among various subset of activities should be recorded
for later reference.

This is a critical part of the analysis as the full appreciation of our
ignorance prior to collecting the data cannot be fully realized without this
record.  Without this concrete evidence, we tend to dismiss our ignorance as
trivial and of no practical importance in planning our reactions to our
findings.  This clearer understanding of what we do not know will better guide
our next actions.

Plan on using at least two sampling periods as it is expected that the activity
granularity will likely need adjusting based on initial results. Also, followup
studies will be critical a behavioral and management changes should be
assessed. This low impact sampling design makes this possible.



# Simulation

## Design

The remainder of this document presents an example data collection in a
simulation study using a realistic design that provides sufficient detail to
assess expected accuracy and precision of the resulting time on task estimates
for the `r length(activities$activity)` activities.

The example has a set of representative activities for `r actor`s.^[ The
software is has the ability to examine multiple job types, job specific
activity lists, and corresponding expected frequencies.].  It simulates asking
a set of `r actor`s to indicate which of the activities listed on the
questionnaire they were doing at the time the question popped up on their
screen. Once the `r actor` makes a selection, that selection is returned as a
result to the collection software, which accumulates the responses for later
analysis.

In the simulation model, these queries and responses can all be processed in
less than a second but, as described below, this simulation is constructed so
that each simulated `r actor` responds `r numbers2words(times_per_month)` times
during the duration of each of the three experiments illustrated below.

The manner in which the questions are presented and the timing of when the
questions are presented are critical aspects of the study design but are not
part of the simulation.

## Simulation Activity Frequencies

The `r actor`s' activities are programmed to occur with the following
frequencies as specified in the CSV file.

```{r activities, echo = FALSE}
activities <- activities[activities$actor_type == actor, ]
caption <- stri_c("List of possible activities for the ",
                  stri_replace_all_fixed(actor, pattern = "_",
                                         replacement = "\\_"),
" with assigned frequencies to be used in simulations.")
activities[ , c("activity", "freq")] %>%
  kbl(digits = 2, col.names = c("Activity", "Frequency"),
      caption = caption, row.names = FALSE) %>%
  kable_styling(latex_options = "HOLD_position") %>%
  kable_material(c("striped", "hover"))

```

## Data Collection Example


For clarity, we demonstrate with the results from sampling one `r actor` one
time and three times.
```{r}
three_samples <- get_and_or_list(make_activity_observation(activities,
                                                           size = 3,
                                                           activities$freq))

```

A single sample: 
`r make_activity_observation(activities, size = 1, activities$freq)` 
Three samples: `r three_samples`.

In the single iteration of the simulation below, we observe how precise the
estimates are if we include `r n_actors` `r actor`s sampled just 
`r times_per_month` times a month for each of `r n_months` months.

Since the algorithm is not actually modeling individual `r actor`s, the order
of sampling has no impact on results, which means we simply take 
`r n_actors * times_per_month * n_months`
(`r n_actors` * `r times_per_month` * `r n_months`)
samples per result set and compare each to the expected values.

```{r small-sample,}
small_sample <- make_activity_observation(
  activities, size = n_actors * times_per_month * n_months, activities$freq)
small_sample_counts <- table(small_sample)
sample_size <- sum(small_sample_counts)
small_sample_duration <- small_sample_counts / sample_size

small_sample_duration <-
  make_complete_sample_durations(small_sample_duration, activities)

activities <- activities[order(activities$activity), ]

caption <- stri_c("List of activities, the frequency in which they were ",
                  "observed, the simulated frequency of the activity ",
                  "(expected), and the absolute difference between the ",
                  "observed and expected frequencies.")

delta <- abs(small_sample_duration$duration - activities$freq)
small_sample_duration %>%
  cbind(data.frame(prob = activities$freq, delta = delta)) %>%
  kbl(digits = 3,
      col.names = c("Description", "Observed", "Expected", "Delta"),
      caption = caption) %>%
  kable_styling(latex_options = "HOLD_position") %>%
  add_header_above(c(" " = 1, "Duration" = 3)) %>%
  kable_material(c("striped", "hover"))

```

(#calculation-of-minutes-per-day) We can estimate the duration in minutes on
each activity if we allocate the `r n_actors * times_per_month * n_months`
observations into one 8 hour day which has 560 minutes.

```{r small-sample-in-minutes}
minutes_per_activity <- small_sample_duration$duration * 560
minutes_per_activity <- data.frame(description = small_sample_duration$description,
                                    minutes = as.numeric(minutes_per_activity))
caption <- stri_c("List of activities and the estimated number of minutes ",
                  "spent on the activity if represented in a single day.")
minutes_per_activity %>%
  kbl(digits = 1, col.names = c("Description", "Minutes"),
      caption = caption) %>%
  kable_styling(latex_options = "HOLD_position") %>%
  kable_material(c("striped", "hover"))

```

I have run this experiment several times and usually the estimates are shown to
be very close as in seen in the _Delta_ column (within about 1 percent).
However, it is more instructive to simulation this experiment many times to
learn what precision we can expect.

<!-- We will start by putting the simulation into a function.  -->

## Results

Figure \@ref(fig:hist-1X) shows a histogram plot of the results of repeating
the simulation of `r n_actors` 
`r stri_replace_all_fixed(actor, pattern = "_", replacement = " ")`s
providing `r times_per_month` responses in each of the 
`r n_months` months of a year `r iterations` times.  Similarly, Figure
\@ref(fig:hist-2X) shows a histogram plot of the similar simulation with 
`r round(n_actors * 2, 0)` 
`r stri_replace_all_fixed(actor, pattern = "_", replacement = " ")`s 
instead of `r n_actors`.

```{r}
this_n_actors <- n_actors
sim_delta_freq <- sim_sample_duration(
  activities,
  size = this_n_actors * times_per_month * n_months,
  iterations = iterations)
gt_1_percent <- sum(sim_delta_freq > 0.01) / length(sim_delta_freq)
gt_2_percent <- sum(sim_delta_freq > 0.02) / length(sim_delta_freq)
gt_3_percent <- sum(sim_delta_freq > 0.03) / length(sim_delta_freq)

hist_1X_cap <-
  stri_c("Simulation of ",
         format(iterations, decimal.mark = ".",
                big.mark = ","),
         " iterations with ",
         this_n_actors, " ",
         stri_replace_all_fixed(actor, pattern = "_", replacement = " "),
         "s sampled ",times_per_month,
         " times per month for one year found ",
         100 * gt_1_percent, "% of durations \nwere greater than ",
         "1 percent away from the ",
         "expected value and ",
         100 * gt_2_percent, "% of durations \nwere greater than ",
         "2 percent away from the ",
         "expected value and ", 100 * gt_3_percent,
         "% of \ndurations were greater than 3 ",
         "percent away from the expected value \nwith the largest ",
         "absolute delta value being ", round(max(sim_delta_freq), 3),
         ".")

```


```{r hist-1X, fig.cap=hist_1X_cap, fig.pos="H"}
data.frame(freq = sim_delta_freq) %>%
  ggplot(aes(x = freq)) +
  geom_histogram(bins = 30) +
  ggtitle(stri_c(this_n_actors, " ",
         stri_replace_all_fixed(actor, pattern = "_", replacement = " "),
         "s sampled ", times_per_month,
                 " times per month for one year")) +
  xlab("Absolute Value of (Expected - Observed) Frequency") +
  ylab("Count")

```
```{r}
this_n_actors <- 2 * n_actors
sim_delta_freq <-
  sim_sample_duration(activities,
                      size = this_n_actors * times_per_month * n_months,
                      iterations = iterations)
gt_1_percent <- sum(sim_delta_freq > 0.01) / length(sim_delta_freq)
gt_2_percent <- sum(sim_delta_freq > 0.02) / length(sim_delta_freq)
gt_3_percent <- sum(sim_delta_freq > 0.03) / length(sim_delta_freq)

hist_2X_cap <-
  stri_c("Simulation of ",
         format(iterations, decimal.mark = ".",
                big.mark = ","),
         " iterations with ",
         this_n_actors, " ",
         stri_replace_all_fixed(actor, pattern = "_", replacement = " "),
         "s sampled ",times_per_month,
         " times per month for one year found ",
         100 * gt_1_percent, "% of durations \nwere greater than ",
         "1 percent away from the ",
         "expected value and ",
         100 * gt_2_percent, "% of \ndurations were greater than ",
         "2 percent away from the ",
         "expected value and \n", 100 * gt_3_percent,
         "% of durations were greater than 3 ",
         "percent away from the expected \nvalue with the largest ",
         "absolute delta value being ", round(max(sim_delta_freq), 3),
         ".")

```
```{r hist-2X, fig.cap=hist_2X_cap, fig.pos="H"}
data.frame(freq = sim_delta_freq) %>%
  ggplot(aes(x = freq)) +
  geom_histogram(bins = 30) +
  ggtitle(paste0(round(this_n_actors, 0), " ",
         stri_replace_all_fixed(actor, pattern = "_", replacement = " "),
         "s sampled ", times_per_month,
                 " times per month for one year.")) +
  xlab("Absolute Value of (Expected - Observed) Frequency") +
  ylab("Count")

```

```{r}
this_n_actors <- 4 * n_actors
sim_delta_freq <-
  sim_sample_duration(activities,
                      size = this_n_actors * times_per_month * n_months,
                      iterations = iterations)
gt_1_percent <- sum(sim_delta_freq > 0.01) / length(sim_delta_freq)
gt_2_percent <- sum(sim_delta_freq > 0.02) / length(sim_delta_freq)
gt_3_percent <- sum(sim_delta_freq > 0.03) / length(sim_delta_freq)

hist_4X_cap <-
  stri_c("Simulation of ",
         format(iterations, decimal.mark = ".",
                big.mark = ","),
         " iterations with ",
         this_n_actors, " ",
         stri_replace_all_fixed(actor, pattern = "_", replacement = " "),
         "s sampled ",times_per_month,
         " times per month for one year found ",
         100 * gt_1_percent, "% of durations \nwere greater than ",
         "1 percent away from the ",
         "expected value and ",
         100 * gt_2_percent, "% of \ndurations were greater than ",
         "2 percent away from the ",
         "expected value and \n", 100 * gt_3_percent,
         "% of durations were greater than 3 ",
         "percent away from the expected \nvalue with the largest ",
         "absolute delta value being ", round(max(sim_delta_freq), 3),
         ".")

```
```{r hist-4X, fig.cap=hist_2X_cap, fig.pos="H"}
data.frame(freq = sim_delta_freq) %>%
  ggplot(aes(x = freq)) +
  geom_histogram(bins = 30) +
  ggtitle(paste0(round(this_n_actors, 0), " ",
         stri_replace_all_fixed(actor, pattern = "_", replacement = " "),
         "s sampled ", times_per_month,
                 " times per month for one year.")) +
  xlab("Absolute Value of (Expected - Observed) Frequency") +
  ylab("Count")

```

`r clearpage()`

## Conclusions

Prior experiments with this cross-sectional activity time estimation have shown
that about 250 samples will provide time estimates for up to 20 activities that
are all accurate and precise within $\pm2$ percent, which should be precise
enough for most management activities. I do not think any amount of sampling
using actor logs can be that accurate because of inherent cognitive bias
introduced by the actors' own expectations.

## Appendix

See the setup code chunk of this vignette to see how to customize this document
to examine results you can expect from your own time on task activity analysis.
You will need to provide the CSV file^[Comparable to
colony_management_defined_activities.csv] with your own actors^[Actors are used
as variable names so must follow rules for naming variables in R. They must
start with a letter, contain only letters, numerals, ".", and "_".],
activities, and activity descriptions in addition to providing your definitions
for the setup chunk definitions indicated below.

```{r key-value-pairs, eval = FALSE, echo = TRUE}
activity_file <-
  system.file("extdata",
              "colony_management_defined_activities.csv",
              package = "crosssectiontimer", lib.loc = NULL,
              mustWork = FALSE)
value_stream_descriptor <- stri_c("husbandry and veterinary care of ",
                                  "colony animals")
actor <- "animal_caretaker" # can be any value in the actor column of the
                            # activity file

# The product of n_actors, times_per_month, and n_months is the size.
# The size should be about 250 for about 20 activities to get the precision
# illustrated herein
n_actors <- 20
times_per_month <- 5
n_months <- 12

iterations <- 5000          # 5000 seems more than adequate for stable results

```
