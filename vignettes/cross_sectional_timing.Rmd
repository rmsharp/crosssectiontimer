---
title: "Test of a Cross-sectional Sampling Timer"
author: "R. Mark Sharp"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2: default
  bookdown::html_document2: default
vignette: >
  %\VignetteEngine{knitr::rmarkdown_notangle} 
  %\VignetteIndexEntry{Interactive Use of nprcgenekeepr} 
  %\usepackage[UTF-8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.pos = 'H')
library("crosssectiontimer")
library(ggplot2)
library(kableExtra)
library(magrittr)
library(rmsutilityr)
library(stringi)

actor <- "implementer"
n_actors <- 20
times_per_month <- 5
iterations <- 5000
n_months <- 12
```

# Purpose

We want to gain insight into how much time is spent on specific activities by 
various actors^[business partners, developers, implementers, Modeling
Center of Excellence (MCOE) personnel, and Model Risk Management (MRM) 
personnel]
responsible for the success of the model deployment life-cycle.
It is anticipated that having time on task data will provide insight in those
areas where where improvement opportunities exist.

Detailed planning regarding questions to be addressed is required to design
which activities^[We expect that well over 
20 percent of time is used in activities that are not directly related to 
model deployment or use. Planning how much detail to collect and how the data
will be analyzed should occur prior to any data being collected.]
should be measured. 
However, the intent of this document is to
propose the use of a cross-sectional study design instead of longitudinal 
design.

A benefit of time on task studies is that they provide hard data regarding how 
time is spent and how much of it directly contributes to our primary goals.
A second benefit is that these data
allow each class of actor to advocate for modifying the way their
time is being used to improve their work environment and productivity.

A longitudinal design calls for representative actors within each group to
log defined activities during their work day.
Logging of time on task is has the problems listed below.   

-   Failure rate in logging activities can be high.
-   Recall of the amount of time spent is often wrong.
-   Preconceptions of how time should be spent biases recalled durations.
-   Keeping, collecting, and analyzing logs requires significant administrative
    overhead to an already administrative heavy environment.
-   The biases introduced from earlier points makes interpretation of results
    difficult.
-   The longitudinal collection of data requires a long data collection
    phase before any analysis can begin. 
-   Variance among different projects is anticipated to be high, which
    further complicates analysis.

The cross-sectional design assumes that we have the ability to send a question
to selected individuals and collect that response.

The cross-sectional design presented and tested below has the following
advantages.

-   Failure rate has far less impact since failure to respond simply 
    initiates another response request.
-   Participants do not need to recall time spent.
-   Administrative overhead is greatly reduced.
-   Bias is minimized.
-   Data could be analyzed within the first month of use.
-   Variance among projects could be assessed and more accurately measured if
    desired. However, I recommend that project differences be ignored
    at the outset.


# Scenario

The remainder of this document presents a simple simulation study using a
realistic design 
and provides sufficient detail to assess expected data quality.

The simulated model has a set of representative activities for `r actor`s.^[
The software is has the ability to examine multiple job types, job specific
activity lists, and corresponding expected frequencies.]. 
It simulates asking a set of `r actor`s to indicate
which of the activities listed on the questionnaire they were doing at the time
the question popped up on their screen. Once the `r actor` makes a selection,
that selection is returned as a result to the collection software, which
cumulates the responses for later analysis. 

In the simulation model, these queries and responses can all be processed in
less than a second but as described below this simulation is constructed so that
each simulated `r actor` responds only `r numbers2words(times_per_month)` 
times during the entire
duration of the experiments illustrated below.

The manner in which the questions are presented and the timing of when the 
questions are presented are critical aspects of the study design, which are 
not discusses further.

## Simulation

The `r actor`s' activities are programmed to occur with the following 
frequencies.

```{r activities, echo = FALSE}
activities <- get_defined_activities()
activities <- activities[activities$actor_type == actor, ]
caption <- stri_c("List of possible activities for the ", actor, 
" with assigend frequencies to be used in simulations.")
activities[ , c("activity", "freq")] %>%
  kbl(digits = 2, col.names = c("Activity", "Frequency"),
      caption = caption, row.names = FALSE) %>%
  kable_styling(latex_options = "HOLD_position") %>%
  kable_material(c("striped", "hover"))

```

For clarity, we demonstrate one and three samples.

A single sample: `r make_activity_observation(actor, size = 1, activities$freq)`   
Three samples: `r get_and_or_list(make_activity_observation(actor, size = 3, activities$freq))`.

Lets see how precise our estimates would be if we included 20 `r actor`s 
sampled just `r times_per_month` times a month for each of `r n_months` months.

Since the algorithm is not actually modeling individual `r actor`s, the order of 
sampling has no impact on results, which means we simply take 
`r n_actors * times_per_month * n_months` 
(`r n_actors` * `r times_per_month` * `r n_months`) compare each result set to 
the expected values.

```{r small-sample,}
small_sample <- make_activity_observation(
  actor, size = n_actors * times_per_month * n_months, activities$freq)
small_sample_counts <- table(small_sample)
sample_size <- sum(small_sample_counts)
small_sample_duration <- small_sample_counts / sample_size

small_sample_duration <- 
  make_complete_sample_durations(small_sample_duration, activities)

activities <- activities[order(activities$activity), ]

caption <- stri_c("List of activities, the frequency in which they were ",
                  "observed, the simulated frequency of the activity ",
                  "(expected), and the absolute difference between the ",
                  "observed and expected frequencies.")

delta <- abs(small_sample_duration$duration - activities$freq)
small_sample_duration %>%
  cbind(data.frame(prob = activities$freq, delta = delta)) %>%
  kbl(digits = 3, 
      col.names = c("Description", "Observed", "Expected", "Delta"),
      caption = caption) %>%
  kable_styling(latex_options = "HOLD_position") %>%
  add_header_above(c(" " = 1, "Duration" = 3)) %>%
  kable_material(c("striped", "hover")) 

```


We can estimate the duration in minutes on each activity if we allocate 
the `r n_actors * times_per_month * n_months` observations into one 8 hour day 
which has 560 minutes.

```{r small-sample-in-minutes}
minutes_per_activity <- small_sample_duration$duration * 560
minutes_per_activity <- data.frame(description = small_sample_duration$description,
                                    minutes = as.numeric(minutes_per_activity))
caption <- stri_c("List of activities and the estimated number of minutes ",
                  "spent on the activity if represented in a single day.")
minutes_per_activity %>%
  kbl(digits = 1, col.names = c("Description", "Minutes"),
      caption = caption) %>%
  kable_styling(latex_options = "HOLD_position") %>%
  kable_material(c("striped", "hover"))

```

I have run this experiment several times and usually the 
estimates are shown to be very close as in seen in the _Delta_ column
(within about 1 percent). However, it is more instructive to simulation this 
experiment many times to learn what precision we can expect.

<!-- We will start by putting the simulation into a function. Note that this is 
not a generic function as it assumes the presence of the globally defined 
objects (__activities$freq__ and __activities$activity__) and it knows about their 
structure.
-->

```{r simulation-function, echo = FALSE}
sim_sample_duration <- 
  function(size = n_actors * times_per_month * n_months, iterations = 1) {
  max_delta <- numeric(iterations)
  for (i in seq_len(iterations)) {
    small_sample <-
      make_activity_observation(actor, size = size,
                                activities$freq)
    small_sample_counts <- table(small_sample)
    sample_size <- sum(small_sample_counts)
    small_sample_duration <- small_sample_counts / sample_size
    small_sample_duration <- 
      make_complete_sample_durations(small_sample_duration, activities)
    max_delta[i] <- 
      max(abs(small_sample_duration$duration - activities$freq))
  }
  max_delta
}

```

Figure \@ref(fig:hist-1200) shows a histogram plot of 
the results of 
repeating the simulation of 20 `r actor`s providing `r times_per_month` responses 
in each of the `r n_months` months of a year `r iterations` times. 
Similarly, Figure \@ref(fig:hist-2400) shows a histogram plot of 
the similar simulation with 40 `r actor`s instead of 20.

```{r}
hist_1200_cap <- paste0("20 ", actor, "s sampled ", times_per_month, 
                        " times per month for one year")

```

```{r hist-1200, fig.cap=hist_1200_cap}
this_n_actors <- n_actors
sim_delta_freq <- sim_sample_duration(
  size = this_n_actors * times_per_month * n_months, iterations = iterations)
gt_2_percent <- sum(sim_delta_freq > 0.02) / length(sim_delta_freq)
gt_3_percent <- sum(sim_delta_freq > 0.03) / length(sim_delta_freq)

data.frame(freq = sim_delta_freq) %>%
ggplot(aes(x = freq)) + 
  geom_histogram(bins = 30) +
  ggtitle(stri_c("Simulation of ", 
                 format(iterations, digits = 0, decimal.mark = ".", 
                        big.mark = ","), 
                 " iterations with ",
                 this_n_actors, " ", actor, "s found ", 
                 100 * gt_2_percent, "% of durations \nwere greater than ", 
                 "2 percent away from the ",
                 "expected value and ", 100 * gt_3_percent, 
                 "% of \ndurations were greater than 3 ", 
                 "percent away from the expected value \nwith the largest ",
                 "absolute delta value being ", round(max(sim_delta_freq), 3), 
                 ".")) +
  xlab("Absolute Value of (Expected - Observed) Frequency") +
  ylab("Count")

```
```{r}
hist_2400_cap <- paste0("40 ", actor, "s sampled ", times_per_month, 
                        " times per month for one year")

```
```{r hist-2400, fig.cap=hist_2400_cap}
this_n_actors <- 2 * n_actors
sim_delta_freq <- 
  sim_sample_duration(size = this_n_actors * times_per_month * n_months, 
                      iterations = iterations)
gt_2_percent <- sum(sim_delta_freq > 0.02) / length(sim_delta_freq)
gt_3_percent <- sum(sim_delta_freq > 0.03) / length(sim_delta_freq)

data.frame(freq = sim_delta_freq) %>%
ggplot(aes(x = freq)) + 
  geom_histogram(bins = 30) +
  ggtitle(stri_c("Simulation of ", 
                 format(iterations, digits = 0, decimal.mark = ".", 
                        big.mark = ","), 
                 " iterations with ",
                 this_n_actors, " ", actor, "s found ", 
                 100 * gt_2_percent, "% of \ndurations were greater than ", 
                 "2 percent away from the ",
                 "expected value and \n", 100 * gt_3_percent, 
                 "% of durations were greater than 3 ", 
                 "percent away from the expected \nvalue with the largest ",
                 "absolute delta value being ", round(max(sim_delta_freq), 3), 
                 ".")) +
  xlab("Absolute Value of (Expected - Observed) Frequency") +
  ylab("Count") 

```

